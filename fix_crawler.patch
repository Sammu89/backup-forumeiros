--- a/crawler.py
+++ b/crawler.py
@@ class DiscoverWorker:
-            # 3) Otherwise fetch the real HTML (following any further redirects)
-            status, html, final_url = await self.fetcher.fetch_text(url, allow_redirects=True)
+            # 3) Otherwise fetch the real HTML (following any further redirects)
+            status, html, final_url = await self.fetcher.fetch_text(url, allow_redirects=True)
@@
-            # — Non-200? —
-            if status != 200 or not html:
-                print(f"[DiscoverWorker {self.id}] HTTP {status}, skipping")
-                await self.state.update_after_fetch(path, False, f"HTTP {status}")
-                continue
+            # — Non-200? —
+            if status != 200 or not html:
+                print(f"[DiscoverWorker {self.id}] HTTP {status}, skipping")
+                await self.state.update_after_fetch(path, False, f"HTTP {status}")
+                continue
+
+            # — Salva o HTML puro localmente (fase 1) —
+            local_path = url_to_local_path(path)
+            try:
+                await asyncio.to_thread(
+                    lambda p, d: open(p, "w", encoding="utf-8").write(d),
+                    local_path, html
+                )
+                print(f"[DiscoverWorker {self.id}] Saved HTML: {local_path}")
+            except Exception as e:
+                traceback.print_exc()
+                print(f"[DiscoverWorker {self.id}] Save error {local_path}: {e}")
+                await self.state.update_after_fetch(path, False, str(e))
+                continue
+
+            # — Marca descoberto APÓS salvar o HTML —
             await self.state.mark_discovered(path)
 
             print(f"[DiscoverWorker {self.id}] {path} → +{added} links")
@@ class DownloadWorker:
-            print(f"[DownloadWorker {self.id}] Fetch for download: {url}")
-            try:
-                # catch Location header first
-                status, _, final_url = await self.fetcher.fetch_text(url, allow_redirects=False)
-                if status in (301, 302) and urlparse(final_url).netloc == BASE_DOMAIN:
-                    # identical redirect handling as above
-                    po, pf = urlparse(url), urlparse(final_url)
-                    src = po.path + (f"?{po.query}" if po.query else "")
-                    dst = pf.path + (f"?{pf.query}" if pf.query else "")
-                    print(f"[DownloadWorker {self.id}] Redirect: {src} → {dst}")
-                    await redirects.add(src, dst)
-                    await self.state.add_url(dst)
-                    await self.state.update_after_fetch(src, True)
-                    continue
-
-                # fetch real content
-                status, html, final_url = await self.fetcher.fetch_text(url, allow_redirects=True)
-            except Exception as e:
-                traceback.print_exc()
-                await self.state.update_after_fetch(path, False, str(e))
-                continue
+            print(f"[DownloadWorker {self.id}] Reading local HTML for: {path}")
+            # Lê o HTML já salvo na fase 1
+            local_html = url_to_local_path(path)
+            try:
+                html = await asyncio.to_thread(
+                    lambda p: open(p, "r", encoding="utf-8").read(),
+                    local_html
+                )
+            except Exception as e:
+                traceback.print_exc()
+                print(f"[DownloadWorker {self.id}] Read error {local_html}: {e}")
+                await self.state.update_after_fetch(path, False, str(e))
+                continue
